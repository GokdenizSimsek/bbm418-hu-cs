# -*- coding: utf-8 -*-
"""BBM418_assignment2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fiOS7aZ8J0s9H3zuJqQNhBBAsyEzTyPS
"""

from google.colab import drive
drive.mount('/content/drive')

import shutil
shutil.copytree("/content/drive/MyDrive/assignment2-dataset/food11", "/content/food11")

import os
import torch
import torch.nn as nn
import torchvision.transforms as transforms
import torchvision.datasets as datasets
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
DATA_DIR = "/content/food11"

# Data augmentation (for only train)
train_transforms = transforms.Compose([
    transforms.Resize(256),
    transforms.RandomCrop(224, padding=8),
    transforms.RandomHorizontalFlip(),
    transforms.ColorJitter(brightness=0.1, contrast=0.1),
    transforms.ToTensor(),
])

val_test_transforms = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

# Load the Datasets
train_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, "train"), transform=train_transforms)
val_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, "validation"), transform=val_test_transforms)
test_dataset = datasets.ImageFolder(os.path.join(DATA_DIR, "test"), transform=val_test_transforms)

"""# PART 1"""

class BasicCNN(nn.Module):
    def __init__(self, num_classes=11):
        super(BasicCNN, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),

            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),

            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )
        self.classifier = nn.Sequential(
            nn.Linear(256 * 28 * 28, 512),
            nn.ReLU(),
            nn.Linear(512, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.relu = nn.ReLU()
        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(out_channels)

        self.skip = nn.Sequential()
        if in_channels != out_channels:
            self.skip = nn.Conv2d(in_channels, out_channels, 1)

    def forward(self, x):
        identity = self.skip(x)
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += identity
        return self.relu(out)

class ResidualCNN(nn.Module):
    def __init__(self, num_classes=11, dropout_prob=0.5):
        super(ResidualCNN, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),
            ResidualBlock(64, 64),
            ResidualBlock(64, 128),
            nn.MaxPool2d(2),
            ResidualBlock(128, 256),
        )

        # Dynamic flatten size
        self.flatten_size = self._get_flatten_size()

        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(self.flatten_size, 512),
            nn.ReLU(),
            nn.Dropout(p=dropout_prob),
            nn.Linear(512, num_classes),
        )

    def _get_flatten_size(self):
        with torch.no_grad():
            dummy = torch.zeros(1, 3, 224, 224)  # Dummy input
            out = self.features(dummy)
            return out.view(1, -1).size(1)

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

def get_dataloaders(batch_size):
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
    return train_loader, val_loader, test_loader

def calculate_accuracy(model, dataloader):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in dataloader:
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)
    return correct / total

def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=50):
    model = model.to(device)
    train_loss_list = []
    train_acc_list = []
    val_acc_list = []

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        for images, labels in train_loader:
            images, labels = images.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * images.size(0)

        epoch_loss = running_loss / len(train_loader.dataset)
        train_accuracy = calculate_accuracy(model, train_loader)
        val_accuracy = calculate_accuracy(model, val_loader)

        train_loss_list.append(epoch_loss)
        train_acc_list.append(train_accuracy)
        val_acc_list.append(val_accuracy)

        print(f"Epoch [{epoch+1}/{num_epochs}] - "
              f"Loss: {epoch_loss:.4f}, "
              f"Train Acc: {train_accuracy:.4f}, "
              f"Val Acc: {val_accuracy:.4f}")

    return train_loss_list, train_acc_list, val_acc_list

"""## **Basic CNN Models with Different Batchsize and Learning Rates**

Batchsize = 32, Learning Rate = 0.001
"""

batch_size = 32
train_loader, val_loader, test_loader = get_dataloaders(batch_size)

model = BasicCNN().to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

train_loss, train_acc, val_acc = train_model(
    model,
    train_loader,
    val_loader,
    criterion,
    optimizer,
    num_epochs=50
)

plt.figure(figsize=(12, 5))

# Train and Validation Accuracy
plt.subplot(1, 2, 1)
plt.plot(train_acc, label='Train Accuracy')
plt.plot(val_acc, label='Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Train and Validation Accuracy (lr=0.001, batch_size=32)")
plt.legend()
plt.grid(True)

# Train Loss
plt.subplot(1, 2, 2)
plt.plot(train_loss, label='Train Loss', color='orange')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""Batchsize = 32, learning rate = 0.0001"""

batch_size = 32
train_loader, val_loader, test_loader = get_dataloaders(batch_size)

model1 = BasicCNN().to(device)

criterion = nn.CrossEntropyLoss()
optimizer1 = torch.optim.Adam(model1.parameters(), lr=0.0001)

train_loss1, train_acc1, val_acc1 = train_model(
    model1,
    train_loader,
    val_loader,
    criterion,
    optimizer1,
    num_epochs=50
)

plt.figure(figsize=(12, 5))

# Train and Validation Accuracy
plt.subplot(1, 2, 1)
plt.plot(train_acc1, label='Train Accuracy')
plt.plot(val_acc1, label='Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Train and Validation Accuracy (lr=0.0001, batch_size=32)")
plt.legend()
plt.grid(True)

# Train Loss
plt.subplot(1, 2, 2)
plt.plot(train_loss1, label='Train Loss', color='orange')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""Batchsize = 32, learning rate = 0.0005"""

model2 = BasicCNN().to(device)

criterion = nn.CrossEntropyLoss()
optimizer2 = torch.optim.Adam(model2.parameters(), lr=0.0005)

train_loss2, train_acc2, val_acc2 = train_model(
    model2,
    train_loader,
    val_loader,
    criterion,
    optimizer2,
    num_epochs=50
)

plt.figure(figsize=(12, 5))

# Train and Validation Accuracy
plt.subplot(1, 2, 1)
plt.plot(train_acc2, label='Train Accuracy')
plt.plot(val_acc2, label='Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Train and Validation Accuracy (lr=0.0005, batch_size=32)")
plt.legend()
plt.grid(True)

# Train Loss
plt.subplot(1, 2, 2)
plt.plot(train_loss2, label='Train Loss', color='orange')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""Batchsize = 64, learning rate = 0.001"""

batch_size_64 = 64
train_loader_64, val_loader_64, test_loader_64 = get_dataloaders(batch_size_64)

model3 = BasicCNN().to(device)

criterion = nn.CrossEntropyLoss()
optimizer3 = torch.optim.Adam(model3.parameters(), lr=0.001)

train_loss3, train_acc3, val_acc3 = train_model(
    model3,
    train_loader_64,
    val_loader_64,
    criterion,
    optimizer3,
    num_epochs=50
)

plt.figure(figsize=(12, 5))

# Train and Validation Accuracy
plt.subplot(1, 2, 1)
plt.plot(train_acc3, label='Train Accuracy')
plt.plot(val_acc3, label='Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Train and Validation Accuracy (lr=0.001, batch_size=64)")
plt.legend()
plt.grid(True)

# Train Loss
plt.subplot(1, 2, 2)
plt.plot(train_loss3, label='Train Loss', color='orange')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""Batchsize = 64, learning rate = 0.0001"""

model4 = BasicCNN().to(device)

criterion = nn.CrossEntropyLoss()
optimizer4 = torch.optim.Adam(model4.parameters(), lr=0.0001)

train_loss4, train_acc4, val_acc4 = train_model(
    model4,
    train_loader_64,
    val_loader_64,
    criterion,
    optimizer4,
    num_epochs=50
)

plt.figure(figsize=(12, 5))

# Train and Validation Accuracy
plt.subplot(1, 2, 1)
plt.plot(train_acc4, label='Train Accuracy')
plt.plot(val_acc4, label='Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Train and Validation Accuracy (lr=0.0001, batch_size=64)")
plt.legend()
plt.grid(True)

# Train Loss
plt.subplot(1, 2, 2)
plt.plot(train_loss4, label='Train Loss', color='orange')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""Batchsize = 64, learning rate = 0.0005"""

model5 = BasicCNN().to(device)

criterion = nn.CrossEntropyLoss()
optimizer5 = torch.optim.Adam(model5.parameters(), lr=0.0005)

train_loss5, train_acc5, val_acc5 = train_model(
    model5,
    train_loader_64,
    val_loader_64,
    criterion,
    optimizer5,
    num_epochs=50
)

plt.figure(figsize=(12, 5))

# Train and Validation Accuracy
plt.subplot(1, 2, 1)
plt.plot(train_acc5, label='Train Accuracy')
plt.plot(val_acc5, label='Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Train and Validation Accuracy (lr=0.0005, batch_size=64)")
plt.legend()
plt.grid(True)

# Train Loss
plt.subplot(1, 2, 2)
plt.plot(train_loss5, label='Train Loss', color='orange')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""**The best result was achieved at the learning rate of 0,0001 and batch size of 32.**

Loss: 0.7653 - Train Acc: 0.7714 - Val Acc: 0.5273
"""

best_model = model1
test_accuracy = calculate_accuracy(best_model, test_loader)
print(f"Test Accuracy (with best validation model): {test_accuracy:.4f}")

"""### Basic Cnn with Dropout Probability"""

class BasicCNN_Dropout(nn.Module):
    def __init__(self, num_classes=11, dropout_prob=0.5):
        super(BasicCNN_Dropout, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(32, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),

            nn.Conv2d(64, 128, 3, padding=1),
            nn.BatchNorm2d(128),
            nn.ReLU(),

            nn.Conv2d(128, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),

            nn.Conv2d(256, 256, 3, padding=1),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2),
        )

        self.classifier = nn.Sequential(
            nn.Linear(256 * 28 * 28, 512),
            nn.ReLU(),
            nn.Dropout(p=dropout_prob),
            nn.Linear(512, num_classes),
        )

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

"""Batchsize = 32, Learning Rate = 0.0001, Dropout Probability = 0.3"""

model_dropout = BasicCNN_Dropout(dropout_prob=0.3).to(device)

criterion = nn.CrossEntropyLoss()
optimizer_dropout = torch.optim.Adam(model_dropout.parameters(), lr=0.0001)

train_loss_dropout, train_acc_dropout, val_acc_dropout = train_model(
    model_dropout,
    train_loader,
    val_loader,
    criterion,
    optimizer_dropout,
    num_epochs=50
)

plt.figure(figsize=(12, 5))

# Train and Validation Accuracy
plt.subplot(1, 2, 1)
plt.plot(train_acc_dropout, label='Train Accuracy')
plt.plot(val_acc_dropout, label='Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Train and Validation Accuracy (lr=0.0001, batch_size=32, dropout=0.3)")
plt.legend()
plt.grid(True)

# Train Loss
plt.subplot(1, 2, 2)
plt.plot(train_loss_dropout, label='Train Loss', color='orange')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""Batchsize = 32, Learning Rate = 0.0001, Dropout Probability = 0.5"""

model_dropout1 = BasicCNN_Dropout(dropout_prob=0.5).to(device)

criterion = nn.CrossEntropyLoss()
optimizer_dropout1 = torch.optim.Adam(model_dropout1.parameters(), lr=0.0001)

train_loss_dropout1, train_acc_dropout1, val_acc_dropout1 = train_model(
    model_dropout1,
    train_loader,
    val_loader,
    criterion,
    optimizer_dropout1,
    num_epochs=50
)

plt.figure(figsize=(12, 5))

# Train and Validation Accuracy
plt.subplot(1, 2, 1)
plt.plot(train_acc_dropout1, label='Train Accuracy')
plt.plot(val_acc_dropout1, label='Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Train and Validation Accuracy (lr=0.0001, batch_size=32, dropout=0.5)")
plt.legend()
plt.grid(True)

# Train Loss
plt.subplot(1, 2, 2)
plt.plot(train_loss_dropout1, label='Train Loss', color='orange')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

test_accuracy_03 = calculate_accuracy(model_dropout, test_loader)
print(f"Test Accuracy for Cnn Model with 0.3 Dropout: {test_accuracy_03:.4f}")

test_accuracy_05 = calculate_accuracy(model_dropout1, test_loader)
print(f"Test Accuracy for Cnn Model with 0.5 Dropout: {test_accuracy_05:.4f}")

"""### Confusion Matrix"""

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

def plot_confusion_matrix(model, dataloader, class_names):
    model.eval()
    preds, labels = [], []
    with torch.no_grad():
        for x, y in dataloader:
            x, y = x.to(device), y.to(device)
            outputs = model(x)
            _, predicted = torch.max(outputs, 1)
            preds += predicted.cpu().tolist()
            labels += y.cpu().tolist()

    cm = confusion_matrix(labels, preds)
    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)
    disp.plot(cmap='Blues', xticks_rotation=45)
    plt.title("Confusion Matrix (Test Set)")
    plt.grid(False)
    plt.show()

# class names are taken from train_dataset
class_names = train_dataset.classes

# Confusion Matrix
plot_confusion_matrix(best_model, test_loader, class_names)

"""## **Residual CNN Models with Different Batchsize and Learning Rates**

Batchsize = 32, Learning Rate = 0.001
"""

batch_size = 32
train_loader, val_loader, test_loader = get_dataloaders(batch_size)

residual_model = ResidualCNN().to(device)

criterion = nn.CrossEntropyLoss()
residual_optimizer = torch.optim.Adam(residual_model.parameters(), lr=0.001)

residual_train_loss, residual_train_acc, residual_val_acc = train_model(
    residual_model,
    train_loader,
    val_loader,
    criterion,
    residual_optimizer,
    num_epochs=50
)

plt.figure(figsize=(12, 5))

# Train and Validation Accuracy
plt.subplot(1, 2, 1)
plt.plot(residual_train_acc, label='Train Accuracy')
plt.plot(residual_val_acc, label='Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Train and Validation Accuracy (lr=0.001, batch_size=32)")
plt.legend()
plt.grid(True)

# Train Loss
plt.subplot(1, 2, 2)
plt.plot(residual_train_loss, label='Train Loss', color='orange')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""Batchsize = 32, Learning Rate = 0.0001"""

residual_model1 = ResidualCNN().to(device)

criterion = nn.CrossEntropyLoss()
residual_optimizer1 = torch.optim.Adam(residual_model1.parameters(), lr=0.0001)

residual_train_loss1, residual_train_acc1, residual_val_acc1 = train_model(
    residual_model1,
    train_loader,
    val_loader,
    criterion,
    residual_optimizer1,
    num_epochs=50
)

plt.figure(figsize=(12, 5))

# Train and Validation Accuracy
plt.subplot(1, 2, 1)
plt.plot(residual_train_acc1, label='Train Accuracy')
plt.plot(residual_val_acc1, label='Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Train and Validation Accuracy (lr=0.0001, batch_size=32)")
plt.legend()
plt.grid(True)

# Train Loss
plt.subplot(1, 2, 2)
plt.plot(residual_train_loss1, label='Train Loss', color='orange')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""Batchsize = 32, Learning Rate = 0.0005"""

residual_model2 = ResidualCNN().to(device)

criterion = nn.CrossEntropyLoss()
residual_optimizer2 = torch.optim.Adam(residual_model2.parameters(), lr=0.0005)

residual_train_loss2, residual_train_acc2, residual_val_acc2 = train_model(
    residual_model2,
    train_loader,
    val_loader,
    criterion,
    residual_optimizer2,
    num_epochs=50
)

plt.figure(figsize=(12, 5))

# Train and Validation Accuracy
plt.subplot(1, 2, 1)
plt.plot(residual_train_acc2, label='Train Accuracy')
plt.plot(residual_val_acc2, label='Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Train and Validation Accuracy (lr=0.0005, batch_size=32)")
plt.legend()
plt.grid(True)

# Train Loss
plt.subplot(1, 2, 2)
plt.plot(residual_train_loss2, label='Train Loss', color='orange')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""Batchsize = 64, Learning Rate = 0.001"""

batch_size_64 = 64
train_loader_64, val_loader_64, test_loader_64 = get_dataloaders(batch_size_64)

residual_model3 = ResidualCNN().to(device)

criterion = nn.CrossEntropyLoss()
residual_optimizer3 = torch.optim.Adam(residual_model3.parameters(), lr=0.001)

residual_train_loss3, residual_train_acc3, residual_val_acc3 = train_model(
    residual_model3,
    train_loader_64,
    val_loader_64,
    criterion,
    residual_optimizer3,
    num_epochs=50
)

plt.figure(figsize=(12, 5))

# Train and Validation Accuracy
plt.subplot(1, 2, 1)
plt.plot(residual_train_acc3, label='Train Accuracy')
plt.plot(residual_val_acc3, label='Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Train and Validation Accuracy (lr=0.001, batch_size=64)")
plt.legend()
plt.grid(True)

# Train Loss
plt.subplot(1, 2, 2)
plt.plot(residual_train_loss3, label='Train Loss', color='orange')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""Batchsize = 64, Learning Rate = 0.0001"""

residual_model4 = ResidualCNN().to(device)

criterion = nn.CrossEntropyLoss()
residual_optimizer4 = torch.optim.Adam(residual_model4.parameters(), lr=0.0001)

residual_train_loss4, residual_train_acc4, residual_val_acc4 = train_model(
    residual_model4,
    train_loader_64,
    val_loader_64,
    criterion,
    residual_optimizer4,
    num_epochs=50
)

plt.figure(figsize=(12, 5))

# Train and Validation Accuracy
plt.subplot(1, 2, 1)
plt.plot(residual_train_acc4, label='Train Accuracy')
plt.plot(residual_val_acc4, label='Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Train and Validation Accuracy (lr=0.0001, batch_size=64)")
plt.legend()
plt.grid(True)

# Train Loss
plt.subplot(1, 2, 2)
plt.plot(residual_train_loss4, label='Train Loss', color='orange')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""Batchsize = 64, Learning Rate = 0.0005"""

residual_model5 = ResidualCNN().to(device)

criterion = nn.CrossEntropyLoss()
residual_optimizer5 = torch.optim.Adam(residual_model5.parameters(), lr=0.0005)

residual_train_loss5, residual_train_acc5, residual_val_acc5 = train_model(
    residual_model5,
    train_loader_64,
    val_loader_64,
    criterion,
    residual_optimizer5,
    num_epochs=50
)

plt.figure(figsize=(12, 5))

# Train and Validation Accuracy
plt.subplot(1, 2, 1)
plt.plot(residual_train_acc5, label='Train Accuracy')
plt.plot(residual_val_acc5, label='Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Train and Validation Accuracy (lr=0.0005, batch_size=64)")
plt.legend()
plt.grid(True)

# Train Loss
plt.subplot(1, 2, 2)
plt.plot(residual_train_loss5, label='Train Loss', color='orange')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""**The best result was achieved at the learning rate of 0.0001 and batch size of 64.**

Loss: 1.5118, Train Acc: 0.5777, Val Acc: 0.4618
"""

residual_best_model = residual_model4
test_accuracy = calculate_accuracy(residual_best_model, test_loader)
print(f"Test Accuracy (with best validation model): {test_accuracy:.4f}")

"""### Residual CNN with Dropout Probability"""

class ResidualCNN_Dropout(nn.Module):
    def __init__(self, num_classes=11, dropout_prob=0.5):
        super(ResidualCNN_Dropout, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(2),
            ResidualBlock(64, 64),
            ResidualBlock(64, 128),
            nn.MaxPool2d(2),
            ResidualBlock(128, 256),
        )

        self.flatten_size = self._get_flatten_size()

        self.classifier = nn.Sequential(
            nn.Flatten(),
            nn.Linear(self.flatten_size, 512),
            nn.ReLU(),
            nn.Dropout(p=dropout_prob),
            nn.Linear(512, num_classes),
        )

    def _get_flatten_size(self):
        with torch.no_grad():
            dummy = torch.zeros(1, 3, 224, 224)  # Dummy input
            out = self.features(dummy)
            return out.view(1, -1).size(1)

    def forward(self, x):
        x = self.features(x)
        x = x.view(x.size(0), -1)
        x = self.classifier(x)
        return x

"""Batchsize = 64, Learning Rate = 0.0001, Dropout Probability = 0.3"""

residual_model_dropout = ResidualCNN_Dropout(dropout_prob=0.3).to(device)

criterion = nn.CrossEntropyLoss()
residual_optimizer_dropout = torch.optim.Adam(residual_model_dropout.parameters(), lr=0.0001)

residual_train_loss_dropout, residual_train_acc_dropout, residual_val_acc_dropout = train_model(
    residual_model_dropout,
    train_loader_64,
    val_loader_64,
    criterion,
    residual_optimizer_dropout,
    num_epochs=50
)

plt.figure(figsize=(12, 5))

# Train and Validation Accuracy
plt.subplot(1, 2, 1)
plt.plot(residual_train_acc_dropout, label='Train Accuracy')
plt.plot(residual_val_acc_dropout, label='Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Train and Validation Accuracy (lr=0.0001, batch_size=32, dropout=0.3)")
plt.legend()
plt.grid(True)

# Train Loss
plt.subplot(1, 2, 2)
plt.plot(residual_train_loss_dropout, label='Train Loss', color='orange')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""Batchsize = 64, Learning Rate = 0.0001, Dropout Probability = 0.5"""

residual_model_dropout1 = ResidualCNN_Dropout(dropout_prob=0.5).to(device)

criterion = nn.CrossEntropyLoss()
residual_optimizer_dropout1 = torch.optim.Adam(residual_model_dropout1.parameters(), lr=0.0001)

residual_train_loss_dropout1, residual_train_acc_dropout1, residual_val_acc_dropout1 = train_model(
    residual_model_dropout1,
    train_loader_64,
    val_loader_64,
    criterion,
    residual_optimizer_dropout1,
    num_epochs=50
)

plt.figure(figsize=(12, 5))

# Train and Validation Accuracy
plt.subplot(1, 2, 1)
plt.plot(residual_train_acc_dropout1, label='Train Accuracy')
plt.plot(residual_val_acc_dropout1, label='Validation Accuracy')
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Train and Validation Accuracy (lr=0.0001, batch_size=32, dropout=0.5)")
plt.legend()
plt.grid(True)

# Train Loss
plt.subplot(1, 2, 2)
plt.plot(residual_train_loss_dropout1, label='Train Loss', color='orange')
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Training Loss")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

"""The best result was achieved at the learning rate of 0.0001 and batch size of 64 and dropout value of 0.5.

Loss: 1.2818, Train Acc: 0.6918, Val Acc: 0.4836
"""

residual_test_accuracy_03 = calculate_accuracy(residual_model_dropout, test_loader)
print(f"Test Accuracy for Residual Cnn Model with 0.3 Dropout: {residual_test_accuracy_03:.4f}")

residual_test_accuracy_05 = calculate_accuracy(residual_model_dropout1, test_loader)
print(f"Test Accuracy for Residual Cnn Model with 0.5 Dropout: {residual_test_accuracy_05:.4f}")

"""### Confusion Matrix for Residual CNN"""

# Confusion Matrix
residual_best_model = residual_model_dropout1
plot_confusion_matrix(residual_best_model, test_loader, class_names)

"""# PART 2"""

import torchvision.models as models
import torch.nn as nn

# Load MobileNetV2 model as pretrained
mobilenet_v2 = models.mobilenet_v2(pretrained=True)

for param in mobilenet_v2.parameters():
    param.requires_grad = False

mobilenet_v2.classifier[1] = nn.Linear(mobilenet_v2.classifier[1].in_features, 11)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
mobilenet_v2 = mobilenet_v2.to(device)

mn_criterion = nn.CrossEntropyLoss()
mn_optimizer = torch.optim.Adam(mobilenet_v2.classifier[1].parameters(), lr=0.0001)

# DataLoaders
batch_size = 32

mn_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)
mn_val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)
mn_test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)

mn_train_loss, mn_train_acc, mn_val_acc = train_model(
    mobilenet_v2,
    mn_train_loader,
    mn_val_loader,
    mn_criterion,
    mn_optimizer,
    num_epochs=50
)

# before fine-tuning
mn_test_accuracy = calculate_accuracy(mobilenet_v2, mn_test_loader)
print(f"Test Accuracy for MobileNetV2 Cnn Model: {mn_test_accuracy:.4f}")

# Open the last blocks of the feature extractor (The last blocks of MobileNetV2 are 17 and 18)
for name, param in mobilenet_v2.features.named_parameters():
    if "17" in name or "18" in name:
        param.requires_grad = True

mn_optimizer_finetune = torch.optim.Adam(filter(lambda p: p.requires_grad, mobilenet_v2.parameters()), lr=0.00001)

# Start fine-tuning training
mn_train_loss_ft, mn_train_acc_ft, mn_val_acc_ft = train_model(
    mobilenet_v2,
    mn_train_loader,
    mn_val_loader,
    mn_criterion,
    mn_optimizer_finetune,
    num_epochs=10  # Fine-tuning is done in fewer epochs
)

# after fine-tuning
mn_test_accuracy = calculate_accuracy(mobilenet_v2, mn_test_loader)
print(f"Test Accuracy for MobileNetV2 Cnn Model: {mn_test_accuracy:.4f}")

# Confusion Matrix
plot_confusion_matrix(mobilenet_v2, mn_test_loader, class_names)